{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding\n",
    "\n",
    "* https://www.coursera.org/learn/natural-language-processing-tensorflow/lecture/1OiEp/introduction\n",
    "\n",
    "Last week, you looked at tokenizing text. Where turn text into sequences of numbers with a number was the value of a key value pair with the key being the word. So for example, you could represent the word TensorFlow with the value nine, and then replace every instance of the word with a nine in a sequence. Using tools and TensorFlow, you are able to process strings to get indices of all the words in a corpus of strings and then convert the strings into matrices of numbers. This is the start of getting sentiment out of your sentences. But right now, it's still just a string of numbers representing words. So from there, how would one actually get sentiment? Well, that's something that can be learned from a corpus of words in much the same way as features were extracted from images. This process is called embedding, with the idea being that words and associated words are clustered as vectors in a multi-dimensional space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-tutorial",
   "language": "python",
   "name": "tf-tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
